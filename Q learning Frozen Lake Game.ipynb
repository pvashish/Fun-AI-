{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Frozen Lake with Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. \n",
    "\n",
    "The task is to retrive the fresbee and come back safe.\n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "\n",
    "SFFF       \n",
    "FHFH       \n",
    "FFFH       \n",
    "HFFG\n",
    "\n",
    "This way there are 16 state action pair and just 4 actions which are - up, down, left, right\n",
    "\n",
    "S: starting point, safe  \n",
    "F: frozen surface, safe  \n",
    "H: hole, fall to your doom  \n",
    "G: goal, where the frisbee is located\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole.  \n",
    "You receive a reward of 1 if you reach the goal, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert state_space_size == 16\n",
    "assert action_space_size == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.03800000000000003\n",
      "2000 :  0.21500000000000016\n",
      "3000 :  0.4100000000000003\n",
      "4000 :  0.5350000000000004\n",
      "5000 :  0.6050000000000004\n",
      "6000 :  0.6610000000000005\n",
      "7000 :  0.6800000000000005\n",
      "8000 :  0.6670000000000005\n",
      "9000 :  0.6570000000000005\n",
      "10000 :  0.6920000000000005\n",
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.55678702 0.506413   0.52140672 0.4871214 ]\n",
      " [0.33941874 0.24596305 0.25443055 0.47698089]\n",
      " [0.41451739 0.29127795 0.25991448 0.27252175]\n",
      " [0.18749375 0.08172572 0.03427734 0.05800487]\n",
      " [0.59911034 0.28341184 0.30877972 0.35580102]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.18535934 0.16141085 0.32000242 0.08515063]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.34401899 0.33345867 0.27120495 0.61851279]\n",
      " [0.40385178 0.65604485 0.46408725 0.46903986]\n",
      " [0.66659043 0.39571696 0.30359662 0.26398548]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3740533  0.50034793 0.73048338 0.6029359 ]\n",
      " [0.75585184 0.78555762 0.73558501 0.73554277]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Created a list to store all the rewards of each episode. This way I can see how the game score changes overtime.\n",
    "rewards_all_episodes = [] \n",
    "\n",
    "# Q-learning algorithm\n",
    "#This for loop is doing the work for everything that happens in a single episode\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    #for each episode I am going to set the state back to 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    #Done variable - keeps track weather or not the episode is finished. So at the start of each episode we initialise\n",
    "    #the variable to false. Simillary we set the reward variable to 0 for initial stage.\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    #This for loop accounts for everything that happens in a timestep within a single episode\n",
    "    for step in range(max_steps_per_episode):       \n",
    "        \n",
    "        # Exploration-exploitation trade-off\n",
    "        #setting exploration threshold to a random number between 0 and 1\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) #exploitate the env and choose the action that has the highest q value from the q table.\n",
    "        else:\n",
    "            action = env.action_space.sample() #explore the snv and sample an action randomly \n",
    "        \n",
    "        #once we get action and put it to the step function\n",
    "        #this function returns a tuple which gives -:\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # After getting the reward, now is the time to update Q-table for Q(s,a) using bellman equation\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        #setting state and reward values\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward        \n",
    "        \n",
    "        #checking if out last action ended the episode or not. If not then we jump out of this episode and move to the next one\n",
    "        if done == True: \n",
    "            break\n",
    "           \n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)    \n",
    "    \n",
    "    #and adding the rewards that we received from the previous episodes\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    avg_reward = sum(r/1000)\n",
    "    print(count, \": \", str(avg_reward))\n",
    "    count += 1000    \n",
    "\n",
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert avg_reward > 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "****You reached the goal!****\n"
     ]
    }
   ],
   "source": [
    "# Watch  agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table.\n",
    "# Run for more episodes to watch longer.\n",
    "\n",
    "for episode in range(1):\n",
    "    state = env.reset()\n",
    "    done = False #same as above, just keeps track of wheather or not the last action ended the episode\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode):        \n",
    "        clear_output(wait=True) #ipython display function whcih clears the output of the cell\n",
    "        #with wait = true, it waits to clear out the output until there is another value to override it\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
